{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient for Tennis\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we have implemented the [Multi-Agent Deep Deterministic Policy Gradient (MADDPG)](https://arxiv.org/abs/1706.02275) reinforcement learning algorithm for a simulated tennis game. This is for the second project of the [Udacity Deep Reinforcement Learning Nanodegree program](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### Setup\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Examine The Tennis Environment\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:unityagents:\n'Academy' started successfully!\nUnity Academy name: Academy\n        Number of Brains: 1\n        Number of External Brains : 1\n        Lesson number : 0\n        Reset Parameters :\n\t\t\nUnity brain name: TennisBrain\n        Number of Visual Observations (per agent): 0\n        Vector Observation space type: continuous\n        Vector Observation space size (per agent): 8\n        Number of stacked Vector Observation: 3\n        Vector Action space type: continuous\n        Vector Action space size (per agent): 2\n        Vector Action descriptions: , \nNumber of agents: 2\nSize of each action: 2\nThere are 2 agents. Each observes a state with length: 24\nThe state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.         -6.65278625 -1.5\n -0.          0.          6.83172083  6.         -0.          0.        ]\n"
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/sebastian/udacity_drl/drlnd-multiagent-project/Tennis_Linux/Tennis.x86_64\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train a MADDPG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Episode 1/2000 -- Buffer Size: 30 -- Average Max Score: 0.000\nEpisode 100/2000 -- Buffer Size: 2898 -- Average Max Score: 0.000\nEpisode 200/2000 -- Buffer Size: 5740 -- Average Max Score: 0.000\nEpisode 300/2000 -- Buffer Size: 8754 -- Average Max Score: 0.005\nEpisode 400/2000 -- Buffer Size: 11594 -- Average Max Score: 0.000\nEpisode 500/2000 -- Buffer Size: 14434 -- Average Max Score: 0.000\nEpisode 600/2000 -- Buffer Size: 17274 -- Average Max Score: 0.000\nEpisode 700/2000 -- Buffer Size: 20114 -- Average Max Score: 0.000\n"
    }
   ],
   "source": [
    "from ddpg import DDPGAgent\n",
    "from utils import ReplayBuffer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DDPG parameters\n",
    "GAMMA = 0.99    # Discount factor\n",
    "TAU = 2.5e-3    # Soft update weight for target networks\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPISODES = 2000         # Total number of episodes\n",
    "BATCH_SIZE = 128            # Batch size for training\n",
    "UPDATE_EVERY = 5            # Number of time steps between training steps\n",
    "NUM_UPDATES = 10            # Number of iterations per training step\n",
    "REPLAY_BUF_SIZE = int(5e5)  # Replay buffer size\n",
    "LR_ACTOR = 2e-4             # Learning rate for actor network\n",
    "LR_CRITIC = 1e-3            # Learning rate for critic network\n",
    "WEIGHT_DECAY_ACTOR = 1e-6   # Regularization for actor network weights\n",
    "WEIGHT_DECAY_CRITIC = 1e-5  # Regularization for critic network weights\n",
    "CLIP_GRAD = 1.0             # Clip value for gradients during training\n",
    "\n",
    "# OU Noise decay parameters\n",
    "INIT_NOISE = 0.5\n",
    "INIT_SIGMA = 0.25\n",
    "MIN_NOISE = 0.05\n",
    "MIN_SIGMA = 0.05\n",
    "NOISE_DECAY = 0.999\n",
    "\n",
    "# Reproducibility by fixing random seed\n",
    "SEED = 4321\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Create agents and replay buffer\n",
    "agents = [DDPGAgent(num_agents=num_agents, gamma=GAMMA, tau=TAU,\n",
    "                    lr_actor=LR_ACTOR, lr_critic=LR_CRITIC, \n",
    "                    weight_decay_actor=WEIGHT_DECAY_ACTOR, \n",
    "                    weight_decay_critic=WEIGHT_DECAY_CRITIC) \n",
    "          for _ in range(num_agents)]\n",
    "buffer = ReplayBuffer(size=REPLAY_BUF_SIZE)\n",
    "\n",
    "# Training Loop\n",
    "noise = INIT_NOISE\n",
    "sigma = INIT_SIGMA\n",
    "best_score = 0\n",
    "scores_vec = []\n",
    "scores_avg_vec = []\n",
    "score_window_size = 100 \n",
    "scores_window = deque(maxlen=score_window_size)\n",
    "idx = 0\n",
    "for ep in range(NUM_EPISODES):\n",
    "\n",
    "    # Reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "\n",
    "    # Run an episode\n",
    "    ep_done = False\n",
    "    while not ep_done:\n",
    "\n",
    "        # Choose actions from the agents\n",
    "        with torch.no_grad():\n",
    "            actions = np.zeros((num_agents, action_size)) # select an action (for each agent)\n",
    "            for aidx in range(num_agents):\n",
    "                state_tensor = torch.tensor(states[aidx], dtype=torch.float)\n",
    "                act_tensor = agents[aidx].act(state_tensor.cuda(), noise=noise)\n",
    "                actions[aidx, :] = act_tensor.cpu().detach().numpy()\n",
    "\n",
    "        # Step the environment and collect data\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        \n",
    "        # Push to the experience buffer\n",
    "        for aidx in range(num_agents):\n",
    "            experience = (states[aidx], actions[aidx], [rewards[aidx]], \n",
    "                        next_states[aidx], [dones[aidx]], \n",
    "                        states, next_states, actions)\n",
    "            buffer.push(experience)\n",
    "\n",
    "        # Update the agents once the buffer is sufficiently full\n",
    "        if len(buffer) > BATCH_SIZE and idx % UPDATE_EVERY == 0:\n",
    "            for _ in range(NUM_UPDATES):\n",
    "                for agent in agents:\n",
    "                    samples = buffer.sample(BATCH_SIZE)\n",
    "                    agent.train(samples)\n",
    "\n",
    "        # Update the time step index\n",
    "        idx += 1\n",
    "\n",
    "        # Roll over next state and terminate\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            ep_done = True\n",
    "\n",
    "    # Reset and decay the OU noise\n",
    "    noise = max(MIN_NOISE, noise*NOISE_DECAY)\n",
    "    sigma = max(MIN_SIGMA, sigma*NOISE_DECAY)\n",
    "    for agent in agents:\n",
    "        agent.noise.reset()\n",
    "        agent.noise.sigma = sigma\n",
    "    \n",
    "    # Update the average scores\n",
    "    max_score = max(scores)\n",
    "    scores_vec.append(max_score)\n",
    "    scores_window.append(max_score)\n",
    "    if (ep+1) % score_window_size == 0 or ep == 0 or ep == NUM_EPISODES-1:\n",
    "        mean_max_score = np.mean(scores_window)\n",
    "        scores_avg_vec.append(mean_max_score)\n",
    "        print(\"\\rEpisode {}/{} -- Buffer Size: {} -- Average Max Score: {:.3f}\".format(\n",
    "              ep+1, NUM_EPISODES, len(buffer), mean_max_score))\n",
    "\n",
    "        # Update the best model\n",
    "        if mean_max_score > best_score:\n",
    "            best_idx = np.argmax(scores)\n",
    "            best_model = copy.deepcopy(agents[best_idx].actor)\n",
    "\n",
    "# Plot the final scores\n",
    "plt.plot(np.arange(len(scores_vec)), scores_vec)\n",
    "plt.plot(np.arange(len(scores_avg_vec))*score_window_size, scores_avg_vec)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Total Return (Maximum of Both Agents)\")\n",
    "plt.legend([\"Raw\", \"Average over {} episodes\".format(score_window_size)])\n",
    "plt.show()\n",
    "\n",
    "# Save the trained agent to file\n",
    "torch.save(best_model.state_dict(), \"trained_actor_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Trained Agent\n",
    "\n",
    "Now we will take the best actor weights from training and use it for self-play. \n",
    "\n",
    "That is, both agents will use the same actor with no added OU noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_actor_weights.pth'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-088273c1cf34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActorNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trained_actor_weights.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Training Loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drlnd/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_actor_weights.pth'"
     ]
    }
   ],
   "source": [
    "# Create a new actor network and assign it the trained weights\n",
    "from networks import ActorNetwork\n",
    "actor = ActorNetwork().to(device)\n",
    "actor.load_state_dict(torch.load(\"trained_actor_weights.pth\"))\n",
    "\n",
    "# Training Loop\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "\n",
    "# Run an episode\n",
    "MAX_STEPS = 10000\n",
    "ep_done = False\n",
    "idx = 0\n",
    "while not ep_done:\n",
    "\n",
    "    # Choose actions for both agents using the trained actor\n",
    "    with torch.no_grad():\n",
    "        actions = np.zeros((num_agents, action_size))\n",
    "        for aidx in range(num_agents):\n",
    "            state_tensor = torch.tensor(states[aidx], dtype=torch.float)\n",
    "            act_tensor = actor.act(state_tensor.cuda())\n",
    "            actions[aidx, :] = act_tensor.cpu().detach().numpy()\n",
    "\n",
    "    # Step the environment and collect data\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    idx += 1\n",
    "\n",
    "    # Termination condition\n",
    "    if np.any(dones) or idx >= MAX_STEPS:\n",
    "        ep_done = True\n",
    "\n",
    "print(\"Final scores after {} steps: {}\".format(idx, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}